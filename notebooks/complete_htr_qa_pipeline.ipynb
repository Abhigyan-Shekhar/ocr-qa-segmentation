{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Complete Handwriting OCR + Q&A Segmentation Pipeline\n",
                "## TrOCR + CRF Integration\n",
                "\n",
                "**What this does:**\n",
                "1. \ud83d\udcf7 Upload handwritten exam image\n",
                "2. \ud83d\udd0d Remove ruled lines (preprocessing)\n",
                "3. \ud83d\udcdd Detect text lines (segmentation)\n",
                "4. \ud83e\udd16 Recognize text with TrOCR (OCR)\n",
                "5. \ud83c\udff7\ufe0f Tag lines with CRF (Q&A segmentation)\n",
                "6. \ud83d\udcca Extract Q&A pairs (structured output)\n",
                "7. \ud83d\udcbe Download JSON output\n",
                "\n",
                "**Complete end-to-end solution!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1\ufe0f\u20e3 Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch pillow opencv-python-headless sklearn-crfsuite\n",
                "\n",
                "import torch\n",
                "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
                "from PIL import Image, ImageDraw\n",
                "import cv2\n",
                "import numpy as np\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "import matplotlib.pyplot as plt\n",
                "from google.colab import files\n",
                "import json\n",
                "import pickle\n",
                "import re\n",
                "from typing import List, Dict, Tuple\n",
                "\n",
                "print(\"\u2705 All dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2\ufe0f\u20e3 Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading TrOCR model...\")\n",
                "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
                "trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n",
                "print(\"\u2705 TrOCR model loaded\")\n",
                "\n",
                "# Note: CRF model will be loaded after upload"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3\ufe0f\u20e3 Upload CRF Model\n",
                "\n",
                "Upload your trained `qa_segmentation_crf_squad.pkl` file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Upload qa_segmentation_crf_squad.pkl:\")\n",
                "uploaded_model = files.upload()\n",
                "\n",
                "# Load CRF model\n",
                "with open('qa_segmentation_crf_squad.pkl', 'rb') as f:\n",
                "    crf_data = pickle.load(f)\n",
                "\n",
                "crf_model = crf_data['model']\n",
                "labels = crf_data['labels']\n",
                "\n",
                "print(\"\\n\u2705 CRF model loaded\")\n",
                "print(f\"   Validation F1: {crf_data['training_stats']['val_f1']:.4f}\")\n",
                "print(f\"   Labels: {labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4\ufe0f\u20e3 Preprocessing Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_ruled_lines(img: np.ndarray, line_thickness_range: Tuple[int, int] = (1, 3)) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Remove horizontal ruled lines from image\n",
                "    \"\"\"\n",
                "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img.copy()\n",
                "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
                "    \n",
                "    # Detect horizontal lines\n",
                "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n",
                "    detected_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
                "    \n",
                "    # Remove lines from original\n",
                "    result = gray.copy()\n",
                "    result[detected_lines > 0] = 255\n",
                "    \n",
                "    return result\n",
                "\n",
                "\n",
                "def segment_lines(img: np.ndarray, remove_lines: bool = True) -> List[Tuple[np.ndarray, Tuple]]:\n",
                "    \"\"\"\n",
                "    Segment image into individual text lines (IMPROVED VERSION)\n",
                "    Returns: [(line_image, (x, y, w, h)), ...]\n",
                "    \"\"\"\n",
                "    # Preprocess\n",
                "    if remove_lines:\n",
                "        img = remove_ruled_lines(img)\n",
                "    \n",
                "    # Denoise\n",
                "    denoised = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
                "    \n",
                "    # Binarize with adaptive threshold for better handwriting\n",
                "    binary = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
                "                                   cv2.THRESH_BINARY_INV, 15, 10)\n",
                "    \n",
                "    # Dilate slightly to connect broken characters\n",
                "    kernel = np.ones((2, 2), np.uint8)\n",
                "    binary = cv2.dilate(binary, kernel, iterations=1)\n",
                "    \n",
                "    # Horizontal projection\n",
                "    h_projection = np.sum(binary, axis=1)\n",
                "    \n",
                "    # IMPROVED: Use median instead of max, lower threshold\n",
                "    threshold = np.median(h_projection[h_projection > 0]) * 0.3\n",
                "    \n",
                "    # Smooth projection to reduce noise\n",
                "    from scipy.ndimage import gaussian_filter1d\n",
                "    h_projection_smooth = gaussian_filter1d(h_projection, sigma=2)\n",
                "    \n",
                "    # Find line boundaries with minimum gap\n",
                "    line_regions = []\n",
                "    in_line = False\n",
                "    start_y = 0\n",
                "    min_gap = 15  # Minimum pixels between lines\n",
                "    last_end = 0\n",
                "    \n",
                "    for y, val in enumerate(h_projection_smooth):\n",
                "        if val > threshold and not in_line:\n",
                "            # Start new line only if minimum gap from last line\n",
                "            if y - last_end > min_gap:\n",
                "                start_y = y\n",
                "                in_line = True\n",
                "        elif val <= threshold and in_line:\n",
                "            # End line if significant height\n",
                "            if y - start_y > 15:  # Minimum line height\n",
                "                line_regions.append((start_y, y))\n",
                "                last_end = y\n",
                "            in_line = False\n",
                "    \n",
                "    # If still in line at end\n",
                "    if in_line and len(h_projection_smooth) - start_y > 15:\n",
                "        line_regions.append((start_y, len(h_projection_smooth)))\n",
                "    \n",
                "    # Extract line images with bounding boxes\n",
                "    lines = []\n",
                "    for start_y, end_y in line_regions:\n",
                "        # Add margin\n",
                "        start_y = max(0, start_y - 5)\n",
                "        end_y = min(img.shape[0], end_y + 5)\n",
                "        \n",
                "        # Vertical projection to find x bounds\n",
                "        line_strip = binary[start_y:end_y, :]\n",
                "        v_projection = np.sum(line_strip, axis=0)\n",
                "        \n",
                "        # Find first and last non-zero columns\n",
                "        non_zero = np.where(v_projection > 0)[0]\n",
                "        if len(non_zero) > 0:\n",
                "            start_x = max(0, non_zero[0] - 10)\n",
                "            end_x = min(img.shape[1], non_zero[-1] + 10)\n",
                "            \n",
                "            # Extract line (from denoised grayscale, not binary)\n",
                "            line_img = denoised[start_y:end_y, start_x:end_x]\n",
                "            bbox = (start_x, start_y, end_x - start_x, end_y - start_y)\n",
                "            lines.append((line_img, bbox))\n",
                "    \n",
                "    return lines\n",
                "\n",
                "print(\"\u2705 Preprocessing functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5\ufe0f\u20e3 TrOCR Recognition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def recognize_line(line_img: np.ndarray) -> str:\n",
                "    \"\"\"\n",
                "    Recognize text in a single line using TrOCR\n",
                "    \"\"\"\n",
                "    # Convert to PIL Image\n",
                "    if len(line_img.shape) == 2:\n",
                "        pil_img = Image.fromarray(line_img).convert('RGB')\n",
                "    else:\n",
                "        pil_img = Image.fromarray(cv2.cvtColor(line_img, cv2.COLOR_BGR2RGB))\n",
                "    \n",
                "    # Process with TrOCR\n",
                "    pixel_values = processor(pil_img, return_tensors='pt').pixel_values\n",
                "    generated_ids = trocr_model.generate(pixel_values)\n",
                "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "    \n",
                "    return text\n",
                "\n",
                "\n",
                "print(\"\u2705 TrOCR recognition function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6\ufe0f\u20e3 CRF Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_line_features(texts: List[str], bboxes: List[Tuple], line_idx: int, prev_label: str = 'O') -> Dict:\n",
                "    \"\"\"\n",
                "    Extract 12 features for CRF from recognized text and bounding box\n",
                "    \"\"\"\n",
                "    text = texts[line_idx].strip()\n",
                "    x, y, w, h = bboxes[line_idx]\n",
                "    \n",
                "    # Calculate indent (x position normalized)\n",
                "    indent_level = x / 100.0  # Normalize by typical margin\n",
                "    \n",
                "    # Vertical gap\n",
                "    vertical_gap = 0\n",
                "    if line_idx > 0:\n",
                "        prev_y = bboxes[line_idx - 1][1] + bboxes[line_idx - 1][3]\n",
                "        vertical_gap = (y - prev_y) / 20.0  # Normalize\n",
                "    \n",
                "    # Text features\n",
                "    words = text.split()\n",
                "    word_count = len(words)\n",
                "    \n",
                "    features = {\n",
                "        # Visual\n",
                "        'indent_level': min(indent_level, 3.0),\n",
                "        'vertical_gap': min(vertical_gap, 5.0),\n",
                "        'x_position': min(indent_level / 3.0, 1.0),\n",
                "        \n",
                "        # Pattern matching\n",
                "        'starts_with_q_marker': bool(re.match(r'^Q\\d+[:.\\s]', text)),\n",
                "        'starts_with_a_marker': text.startswith('A:') or text.startswith('A. '),\n",
                "        'starts_with_number': text and text[0].isdigit(),\n",
                "        \n",
                "        # Textual\n",
                "        'ends_with_question': text.endswith('?'),\n",
                "        'has_colon_start': ':' in text[:15],\n",
                "        'is_capitalized': text and text[0].isupper(),\n",
                "        'word_count': min(word_count, 20),\n",
                "        'line_length': min(len(text), 100),\n",
                "        \n",
                "        # Context\n",
                "        'prev_label': prev_label,\n",
                "    }\n",
                "    \n",
                "    return features\n",
                "\n",
                "\n",
                "def texts_to_crf_features(texts: List[str], bboxes: List[Tuple]) -> List[Dict]:\n",
                "    \"\"\"\n",
                "    Convert recognized texts to CRF feature format\n",
                "    \"\"\"\n",
                "    features_sequence = []\n",
                "    prev_label = 'O'\n",
                "    \n",
                "    for idx in range(len(texts)):\n",
                "        features = extract_line_features(texts, bboxes, idx, prev_label)\n",
                "        features_sequence.append(features)\n",
                "    \n",
                "    return features_sequence\n",
                "\n",
                "\n",
                "print(\"\u2705 CRF feature extraction functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7\ufe0f\u20e3 Q&A Pair Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_qa_pairs(texts: List[str], tags: List[str]) -> List[Dict]:\n",
                "    \"\"\"\n",
                "    Extract Q&A pairs from tagged lines\n",
                "    \"\"\"\n",
                "    pairs = []\n",
                "    current_q = []\n",
                "    current_a = []\n",
                "    q_number = 0\n",
                "    \n",
                "    for text, tag in zip(texts, tags):\n",
                "        if tag == 'B-Q':\n",
                "            # Save previous pair\n",
                "            if current_q and current_a:\n",
                "                pairs.append({\n",
                "                    'question_number': q_number,\n",
                "                    'question': ' '.join(current_q),\n",
                "                    'answer': ' '.join(current_a)\n",
                "                })\n",
                "            # Start new question\n",
                "            current_q = [text.strip()]\n",
                "            current_a = []\n",
                "            q_number += 1\n",
                "            \n",
                "        elif tag == 'I-Q':\n",
                "            current_q.append(text.strip())\n",
                "            \n",
                "        elif tag == 'B-A':\n",
                "            current_a = [text.strip()]\n",
                "            \n",
                "        elif tag == 'I-A':\n",
                "            current_a.append(text.strip())\n",
                "    \n",
                "    # Don't forget last pair\n",
                "    if current_q and current_a:\n",
                "        pairs.append({\n",
                "            'question_number': q_number,\n",
                "            'question': ' '.join(current_q),\n",
                "            'answer': ' '.join(current_a)\n",
                "        })\n",
                "    \n",
                "    return pairs\n",
                "\n",
                "\n",
                "print(\"\u2705 Q&A extraction function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8\ufe0f\u20e3 Upload & Process Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Upload your handwritten exam image:\")\n",
                "uploaded_img = files.upload()\n",
                "\n",
                "if uploaded_img:\n",
                "    img_filename = list(uploaded_img.keys())[0]\n",
                "    print(f\"\\n\u2705 Uploaded: {img_filename}\")\n",
                "    \n",
                "    # Load image\n",
                "    img = cv2.imread(img_filename, cv2.IMREAD_GRAYSCALE)\n",
                "    print(f\"   Image size: {img.shape[1]}x{img.shape[0]} pixels\")\n",
                "    \n",
                "    # Display original\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    plt.title(\"Original Image\")\n",
                "    plt.axis('off')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No image uploaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9\ufe0f\u20e3 Step 1: Line Segmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Segmenting text lines...\")\n",
                "lines = segment_lines(img, remove_lines=True)\n",
                "print(f\"\u2705 Found {len(lines)} text lines\")\n",
                "\n",
                "# Visualize line detection\n",
                "img_display = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
                "for i, (line_img, (x, y, w, h)) in enumerate(lines):\n",
                "    cv2.rectangle(img_display, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
                "    cv2.putText(img_display, f'Line {i+1}', (x, y-5), \n",
                "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
                "\n",
                "plt.figure(figsize=(14, 10))\n",
                "plt.imshow(cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB))\n",
                "plt.title(f\"Detected {len(lines)} Lines\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udd1f Step 2: OCR with TrOCR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Recognizing text with TrOCR...\\n\")\n",
                "recognized_texts = []\n",
                "bboxes = []\n",
                "\n",
                "for i, (line_img, bbox) in enumerate(lines):\n",
                "    print(f\"Processing line {i+1}/{len(lines)}...\", end=' ')\n",
                "    text = recognize_line(line_img)\n",
                "    recognized_texts.append(text)\n",
                "    bboxes.append(bbox)\n",
                "    print(f\"\u2713 Text: {text}\")\n",
                "\n",
                "print(f\"\\n\u2705 OCR complete! Recognized {len(recognized_texts)} lines\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1\ufe0f\u20e31\ufe0f\u20e3 Step 3: CRF Q&A Segmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Extracting features for CRF...\")\n",
                "crf_features = texts_to_crf_features(recognized_texts, bboxes)\n",
                "print(f\"\u2705 Extracted features for {len(crf_features)} lines\")\n",
                "\n",
                "print(\"\\nPredicting Q&A tags with CRF...\")\n",
                "predicted_tags = crf_model.predict([crf_features])[0]\n",
                "print(f\"\u2705 Predicted tags\")\n",
                "\n",
                "# Display predictions\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TAGGED OUTPUT\")\n",
                "print(\"=\"*80)\n",
                "for text, tag in zip(recognized_texts, predicted_tags):\n",
                "    print(f\"[{tag:5s}] {text}\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1\ufe0f\u20e32\ufe0f\u20e3 Step 4: Extract Q&A Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Extracting Q&A pairs...\")\n",
                "qa_pairs = extract_qa_pairs(recognized_texts, predicted_tags)\n",
                "print(f\"\u2705 Found {len(qa_pairs)} question-answer pairs\\n\")\n",
                "\n",
                "# Display pairs\n",
                "print(\"=\"*80)\n",
                "print(\"QUESTION-ANSWER PAIRS\")\n",
                "print(\"=\"*80)\n",
                "for pair in qa_pairs:\n",
                "    print(f\"\\nQ{pair['question_number']}: {pair['question']}\")\n",
                "    print(f\"A: {pair['answer']}\")\n",
                "    print(\"-\" * 80)\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1\ufe0f\u20e33\ufe0f\u20e3 Save & Download Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output JSON\n",
                "output = {\n",
                "    'input_image': img_filename,\n",
                "    'num_lines': len(recognized_texts),\n",
                "    'num_qa_pairs': len(qa_pairs),\n",
                "    'qa_pairs': qa_pairs,\n",
                "    'raw_text': ' '.join(recognized_texts),\n",
                "    'tagged_lines': [\n",
                "        {'text': text, 'tag': tag}\n",
                "        for text, tag in zip(recognized_texts, predicted_tags)\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Save JSON\n",
                "output_filename = 'qa_extraction_results.json'\n",
                "with open(output_filename, 'w') as f:\n",
                "    json.dump(output, f, indent=2)\n",
                "\n",
                "print(f\"\\n\u2705 Results saved to: {output_filename}\")\n",
                "print(f\"\\n\ud83d\udcca Summary:\")\n",
                "print(f\"   Lines detected: {output['num_lines']}\")\n",
                "print(f\"   Q&A pairs found: {output['num_qa_pairs']}\")\n",
                "\n",
                "# Download\n",
                "files.download(output_filename)\n",
                "print(f\"\\n\u2705 Downloaded: {output_filename}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \u2705 Complete Pipeline Success!\n",
                "\n",
                "**What was accomplished:**\n",
                "1. \u2705 Preprocessed image (removed ruled lines)\n",
                "2. \u2705 Segmented text lines\n",
                "3. \u2705 Recognized handwriting with TrOCR\n",
                "4. \u2705 Tagged lines with CRF (Q vs A)\n",
                "5. \u2705 Extracted structured Q&A pairs\n",
                "6. \u2705 Exported JSON output\n",
                "\n",
                "**Output includes:**\n",
                "- `qa_pairs`: Structured question-answer pairs\n",
                "- `raw_text`: Full recognized text\n",
                "- `tagged_lines`: Line-by-line with BIO tags\n",
                "\n",
                "**Ready for production use!** \ud83c\udf89"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "colab": {
            "provenance": []
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}